{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#API + url init\n",
    "import getpass\n",
    "import os\n",
    "\n",
    "API_KEY = ''\n",
    "uri = \"\"\n",
    "\n",
    "\n",
    "#genai.configure(api_key=API_KEY)\n",
    "\n",
    "#from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "google_api_key = API_KEY\n",
    "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
    "\n",
    "#embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "#vector = embeddings.embed_documents([\"hello, world!\", \"This is my second document\" , \"This is my third document\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n",
      "768\n",
      "LLM is explicitly disabled. Using MockLLM.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16423/2111427378.py:19: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n"
     ]
    }
   ],
   "source": [
    "#Initialize variables + mongodb\n",
    "import pymongo\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.core import ServiceContext\n",
    "\n",
    "DB_NAME = \"langchain_demo\"\n",
    "COLLECTION_NAME = 'collection_of_text_blobs'\n",
    "INDEX_NAME = 'Indexx'\n",
    "\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(uri)\n",
    "print (\"Atlas client initialized\")\n",
    "\n",
    "embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "\n",
    "vector = embed_model.get_text_embedding(\"Vector Search with MongoDB\")\n",
    "print(len(vector))\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = INDEX_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 chunks from './sample_files/'\n"
     ]
    }
   ],
   "source": [
    "#Embeds + uploads context files to mongoDB\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "data_dir = \"./sample_files/\"\n",
    "\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\")\n",
    "\n",
    "\n",
    "\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    docs, \n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count before delete : 3\n",
      "Deleted docs : 3\n"
     ]
    }
   ],
   "source": [
    "#Delete embedding files to add new context into the db\n",
    "database = mongodb_client[DB_NAME]\n",
    "collection = database[COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Problems with the Pharmacy:**\n",
       "\n",
       "* **Slow customer service due to lack of electronic billing machine:** Leads to long wait times and frustrations for customers.\n",
       "* **Unlabeled and disorganized medicine:** Makes it difficult to locate specific medications, wasting time and effort.\n",
       "\n",
       "**Solutions:**\n",
       "\n",
       "**Problem 1: Slow Customer Service**\n",
       "\n",
       "* **Implement an electronic billing system:** Streamline the billing process, reducing wait times and improving customer satisfaction.\n",
       "* **Train cashiers on efficient billing procedures:** Ensure cashiers are proficient in operating the electronic billing system to expedite transactions.\n",
       "* **Consider self-service checkout kiosks:** Allow customers to scan and pay for their medications independently, freeing up cashiers for more complex tasks.\n",
       "\n",
       "**Problem 2: Unlabeled and Disorganized Medicine**\n",
       "\n",
       "* **Create a clear and logical labeling system:** Use color-coding, alphabetical order, or other methods to categorize and identify medications.\n",
       "* **Implement a medication inventory management system:** Track medication inventory levels and optimize storage to ensure supplies are readily available.\n",
       "* **Arrange medications by category or prescription:** Make it easier for customers and staff to locate specific medications quickly and efficiently.\n",
       "* **Provide clear instructions on medication storage:** Ensure medications are stored in a safe and accessible manner."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Main driver Code \n",
    "\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "\n",
    "\n",
    "query = \"what are some problems with the pharmacy? Give me solutions to deal with it\"\n",
    "\n",
    "\n",
    "def generate_embedding(quer):\n",
    "  temp = embed_model.get_text_embedding(quer)\n",
    "  return temp\n",
    "\n",
    "mongodb_client = pymongo.MongoClient(uri)\n",
    "db = client.langchain_demo\n",
    "collection = db.collection_of_text_blobs\n",
    "\n",
    "results = collection.aggregate([\n",
    "  {\n",
    "    \"$vectorSearch\": {\n",
    "      \"queryVector\": generate_embedding(query),\n",
    "      \"path\": \"embedding\",\n",
    "      \"numCandidates\": 50,\n",
    "      \"limit\": 1,\n",
    "      \"index\": \"RAGIndexing\",\n",
    "    }\n",
    "  }\n",
    "])\n",
    "\n",
    "context = \"\"\n",
    "for document in results:\n",
    "    print(type(document))\n",
    "    print(f'Text present: {document[\"text\"]}\\n')\n",
    "    context += document[\"text\"]\n",
    "\n",
    "print(context)\n",
    "\n",
    "model=genai.GenerativeModel(\"gemini-pro\")\n",
    "#response = model.generate_content(query, stream=True)\n",
    "response = model.generate_content([context, query], stream=True)\n",
    "\n",
    "buffer = []\n",
    "for chunk in response:\n",
    "    for part in chunk.parts:\n",
    "        buffer.append(part.text)\n",
    "    clear_output()\n",
    "    display(Markdown(''.join(buffer)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updating mongoDB so that the embeddings are indexed\n",
    "{\n",
    "  \"mappings\": {\n",
    "    \"dynamic\": true,\n",
    "    \"fields\": {\n",
    "      \"embedding\": {\n",
    "        \"dimensions\": 768,\n",
    "        \"similarity\": \"dotProduct\",\n",
    "        \"type\": \"knnVector\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "llm = Gemini(model=\"models/gemini-pro\")\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\n",
    "query_llm = index.as_query_engine()\n",
    "print(query_llm.query(\"What does this report outline?\"))\n",
    "\"\"\"\n",
    "# Import the necessary libararies\n",
    "import os\n",
    "from pymongo import MongoClient\n",
    "from llama_index.embeddings.gemini import GeminiEmbedding\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "\n",
    "mongodb_client = MongoClient(uri)\n",
    "\n",
    "\n",
    "embed_model = GeminiEmbedding(model_name=\"models/embedding-001\")\n",
    "\n",
    "llm = Gemini(model=\"models/gemini-pro\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=llm)\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = INDEX_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "index = VectorStoreIndex.from_vector_store(vector_store=vector_store, service_context=service_context)\n",
    "\n",
    "query_llm = index.as_query_engine()\n",
    "\n",
    "            \n",
    "prompt = \"what does Complicated dosing schedules lead to?\"\n",
    "\n",
    "answer = query_llm.query(prompt)\n",
    "\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LoadData \n",
    "\n",
    "from pymongo import MongoClient\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.vectorstores import MongoDBAtlasVectorSearch \n",
    "\n",
    "#from langchain.llms import GoogleGenerativeAIEmbeddings\n",
    "#from langchain.llms import gemini\n",
    "from langchain.chains import RetrievalQA\n",
    "import gradio as gr\n",
    "from gradio.themes.base import Base\n",
    "import google.generativeai as genai\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "\n",
    "#Initializing the database(Files samplespace)\n",
    "client = MongoClient(uri)\n",
    "dbName = \"langchain_demo\"\n",
    "collectionName = \"collection_of_text_blobs\"\n",
    "collectionn = client[dbName][collectionName]\n",
    "\n",
    "#dataloader from directory\n",
    "#loader = DirectoryLoader( './sample_files/', glob = \"./*txt\", show_progress = True)\n",
    "loader = TextLoader(\"./sample_files/sample1.txt\")\n",
    "data = loader.load()\n",
    "print(data)\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "#vectorStore = MongoDBAtlasVectorSearch.from_documents(data, embeddings, collection=collectionn)\n",
    "vector = embeddings.embed_documents(data)\n",
    "print(vector)\n",
    "\n",
    "\n",
    "#embeddings_list = list(embeddings)\n",
    "#embeddings_dict = [embedding[0].to_dict() for embedding in embeddings_list]\n",
    "\n",
    "\n",
    "\n",
    "#vectorStore = MongoDBAtlasVectorSearch.from_documents(data, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = genai.GenerativeModel(model_name='gemini-pro')\n",
    "response = model.generate_content(prompt, stream=True)\n",
    "embeddings = GoogleGenerativeAIEmbeddings\n",
    "\n",
    "google.generativeai.embed_content(\n",
    "    model: \"models/text-embedding-004\" ,\n",
    "    content: (content_types.ContentType | Iterable[content_types.ContentType]),\n",
    "    task_type: (EmbeddingTaskTypeOptions | None) = None,\n",
    "    title: (str | None) = None,\n",
    "    output_dimensionality: (int | None) = None,\n",
    "    client: glm.GenerativeServiceClient = None,\n",
    "    request_options: (dict[str, Any] | None) = None\n",
    ") -> (text_types.EmbeddingDict | text_types.BatchEmbeddingDict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
