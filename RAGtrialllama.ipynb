{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 3\n",
      "[Document(id_='37bd3c49-466d-4628-a612-5a9ead4f1562', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Lateb\\\\OneDrive\\\\Desktop\\\\CODING\\\\PharmacyProblemAnalyzer-Gemini-main\\\\PharmacyProblemAnalyzer-Gemini-main\\\\sample_files\\\\sample1.txt', 'file_name': 'sample1.txt', 'file_type': 'text/plain', 'file_size': 229, 'creation_date': '2024-05-19', 'last_modified_date': '2024-05-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"The issues with the pharmacy are as follows: 1. The cashier doesn't have an electronic billing machine so customer service is extremely slow. 2. The medicine is not labeled or arranged in order so searching for it is very tiring.\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='f45b9ecb-97ca-49af-ac10-b8b368c00835', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Lateb\\\\OneDrive\\\\Desktop\\\\CODING\\\\PharmacyProblemAnalyzer-Gemini-main\\\\PharmacyProblemAnalyzer-Gemini-main\\\\sample_files\\\\sample2.txt', 'file_name': 'sample2.txt', 'file_type': 'text/plain', 'file_size': 391, 'creation_date': '2024-05-19', 'last_modified_date': '2024-05-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Medication Adherence:Non-adherence: Patients failing to follow medication schedules can lead to treatment failure and potential health complications.Complexity of Regimens: Complicated dosing schedules or pill combinations can lead to confusion and missed medications.Lack of Patient Education: Inadequate understanding of medication purpose and side effects can contribute to non-adherence.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'), Document(id_='12f99d76-5ee5-418a-ab7c-1156a6f26cec', embedding=None, metadata={'file_path': 'c:\\\\Users\\\\Lateb\\\\OneDrive\\\\Desktop\\\\CODING\\\\PharmacyProblemAnalyzer-Gemini-main\\\\PharmacyProblemAnalyzer-Gemini-main\\\\sample_files\\\\sample3.txt', 'file_name': 'sample3.txt', 'file_type': 'text/plain', 'file_size': 338, 'creation_date': '2024-05-19', 'last_modified_date': '2024-05-19'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]\n",
      "Loaded 3 chunks from './sample_files/'\n"
     ]
    }
   ],
   "source": [
    "#to load documents from a folder \n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "data_dir = \"./sample_files/\"\n",
    "\n",
    "docs = SimpleDirectoryReader(\n",
    "        input_dir=data_dir\n",
    ").load_data()\n",
    "\n",
    "print('Number of pages:', len(docs))\n",
    "print(docs)\n",
    "\n",
    "print (f\"Loaded {len(docs)} chunks from '{data_dir}'\") #change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up variables\n",
    "\n",
    "uri = \"mongodb+srv://geminiuser:1234@cluster0.nurmebz.mongodb.net/?retryWrites=true&w=majority&appName=Cluster0\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atlas client initialized\n",
      "768\n",
      "LLM is explicitly disabled. Using MockLLM.\n",
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lateb\\AppData\\Local\\Temp\\ipykernel_22244\\4290373620.py:29: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.29.235:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:27:25] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:27:26] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:192.168.29.235 - - [24/May/2024 02:27:28] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "INFO:werkzeug:192.168.29.235 - - [24/May/2024 02:27:29] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 chunks from uploaded files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:31:10] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#Api to embed + upload files onto mongodb\n",
    "from flask import Flask, request, jsonify\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pymongo\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "\n",
    "DB_NAME = \"langchain_demo\"\n",
    "COLLECTION_NAME = 'collection_of_text_blobs'\n",
    "INDEX_NAME = 'Indexx'\n",
    "mongodb_client = pymongo.MongoClient(uri)\n",
    "db = mongodb_client[DB_NAME]\n",
    "collection = db[COLLECTION_NAME]\n",
    "\n",
    "print(\"Atlas client initialized\")\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"./sentence-transformers\")\n",
    "\n",
    "vector = embed_model.get_text_embedding(\"Vector Search with MongoDB\")\n",
    "print(len(vector))\n",
    "\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm=None)\n",
    "\n",
    "from llama_index.vector_stores.mongodb import MongoDBAtlasVectorSearch\n",
    "from llama_index.core import StorageContext\n",
    "\n",
    "vector_store = MongoDBAtlasVectorSearch(mongodb_client = mongodb_client,\n",
    "                                 db_name = DB_NAME, collection_name = COLLECTION_NAME,\n",
    "                                 index_name  = INDEX_NAME)\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "\n",
    "@app.route('/upload', methods=['POST'])\n",
    "def upload_documents():\n",
    "    temp_dir = \"./temp_files\"\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "    files = request.files.getlist(\"files\")\n",
    "    for file in files:\n",
    "        file_path = os.path.join(temp_dir, file.filename)\n",
    "        file.save(file_path)\n",
    "\n",
    "    docs = SimpleDirectoryReader(input_dir=temp_dir).load_data()\n",
    "    print(f\"Loaded {len(docs)} chunks from uploaded files.\")\n",
    "\n",
    "    index = VectorStoreIndex.from_documents(docs, storage_context=storage_context, service_context=service_context)\n",
    "\n",
    "    for doc in docs:\n",
    "        embedding = embed_model.get_text_embedding(doc.text)\n",
    "        collection.insert_one({\"text\": doc.text, \"embedding\": embedding})\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "    return jsonify({\"message\": f\"Successfully loaded {len(docs)} documents into MongoDB.\"}), 200\n",
    "\n",
    "def generate_embedding(text):\n",
    "    return embed_model.get_text_embedding(text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document count before delete : 6\n",
      "Deleted docs : 6\n"
     ]
    }
   ],
   "source": [
    "#Delete embedding files to add new context into the db\n",
    "database = mongodb_client[DB_NAME]\n",
    "collection = database[COLLECTION_NAME]\n",
    "\n",
    "doc_count = collection.count_documents (filter = {})\n",
    "print (f\"Document count before delete : {doc_count:,}\")\n",
    "\n",
    "result = collection.delete_many(filter= {})\n",
    "print (f\"Deleted docs : {result.deleted_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up the tokenizer\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "\n",
    "Settings.llm = HuggingFaceLLM(\n",
    "    context_window=2048,\n",
    "    max_new_tokens=512,\n",
    "    generate_kwargs={\"temperature\": 0.1, \"do_sample\": False},\n",
    "    tokenizer_name=\"tinyllama-tokenizer\",\n",
    "    model_name=\"tinyllama-model\",\n",
    "    tokenizer_kwargs={\"max_length\": 2048},\n",
    "    model_kwargs={\"torch_dtype\": torch.float16}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.29.235:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:20<00:00,  6.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n",
      "<class 'dict'>\n",
      "Text present: Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.\n",
      "\n",
      "Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [24/May/2024 02:44:16] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "#api to query\n",
    "from flask import Flask, request, jsonify\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "import pymongo\n",
    "from IPython.display import Markdown, clear_output, display\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query_model():\n",
    "    query_data = request.json\n",
    "    query = query_data.get(\"query\")\n",
    "\n",
    "\n",
    "    client = pymongo.MongoClient(uri)\n",
    "    db = client.langchain_demo\n",
    "    collection = db.collection_of_text_blobs\n",
    "\n",
    "    def generate_embedding(quer):\n",
    "        temp = embed_model.get_text_embedding(quer)\n",
    "        return temp\n",
    "\n",
    "    results = collection.aggregate([\n",
    "        {\n",
    "            \"$vectorSearch\": {\n",
    "                \"queryVector\": generate_embedding(query),\n",
    "                \"path\": \"embedding\",\n",
    "                \"numCandidates\": 50,\n",
    "                \"limit\": 1,\n",
    "                \"index\": \"RAGIndexing\",\n",
    "            }\n",
    "        }\n",
    "    ])\n",
    "\n",
    "    context = \"\"\n",
    "    for document in results:\n",
    "        print(type(document))\n",
    "        print(f'Text present: {document[\"text\"]}\\n')\n",
    "        context += document[\"text\"]\n",
    "\n",
    "    print(\"Query worked\")\n",
    "    print(context)\n",
    "\n",
    "    pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "    print(\"Pipeline code worked\")\n",
    "\n",
    "\n",
    "    def prompt_tinyllama(prompt, system_prompt=\"\"):\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]\n",
    "        prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "        return outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1]\n",
    "\n",
    "\n",
    "    prompt = f\"With the following context- {context}\\nAnswer the following query {query}\"\n",
    "    print(\"Querying: \"+ prompt)\n",
    "    system_prompt = \"You are an expert in this field and always provide detailed and accurate answers\"\n",
    "    response = prompt_tinyllama(prompt, system_prompt)\n",
    "    return jsonify({\"response\": response})\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host=\"0.0.0.0\", port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "Text present: Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.\n",
      "\n",
      "Query worked\n",
      "Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.\n",
      "Pipeline code worked\n",
      "Querying: With the following context- Long Wait Times: Understaffing: Insufficient pharmacy staff can lead to long waiting times for patients seeking medication or consultation. Repetitive Tasks: Manual processes and excessive paperwork can slow down pharmacists and technicians. Inefficient Workflow: Poorly designed workflow can create bottlenecks and hinder timely service.\n",
      "Answer the following query what are some problems with the pharmacy? Give me solutions to deal with it\n",
      "\n",
      "Yes, I can provide you with some potential solutions to address the issues of long wait times, insufficient staff, repetitive tasks, manual processes, and inefficient workflow in a pharmacy.\n",
      "\n",
      "1. Increase Staffing: One of the most effective solutions to alleviate the problem of low staffing is to hire more pharmacists and technicians. This can be achieved by offering competitive salaries, benefits, and training programs.\n",
      "\n",
      "2. Streamline Workflow: To improve workflow and reduce wait times, pharmacies can automate manual processes such as filling prescriptions, processing refills, and answering questions. This can be done by implementing electronic health records (EHRs) and other digital solutions.\n",
      "\n",
      "3. Increase Efficiency: One way to reduce wait times and improve efficiency is to streamline the pharmacy's workflow. This can be done by implementing automation technologies such as barcode scanning and automated prescription dispensing systems.\n",
      "\n",
      "4. Provide Patient Education: Patients who come to the pharmacy for medication or consultation should receive information about the medication, its dosage, and any side effects.\n"
     ]
    }
   ],
   "source": [
    "#code to process queries and return answers\n",
    "\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "client = pymongo.MongoClient(uri)\n",
    "db = client.langchain_demo\n",
    "collection = db.collection_of_text_blobs\n",
    "\n",
    "query = \"what are some problems with the pharmacy? Give me solutions to deal with it\"\n",
    "\n",
    "def generate_embedding(quer):\n",
    "    temp = embed_model.get_text_embedding(quer)\n",
    "    return temp\n",
    "\n",
    "results = collection.aggregate([\n",
    "    {\n",
    "        \"$vectorSearch\": {\n",
    "            \"queryVector\": generate_embedding(query),\n",
    "            \"path\": \"embedding\",\n",
    "            \"numCandidates\": 50,\n",
    "            \"limit\": 1,\n",
    "            \"index\": \"RAGIndexing\",\n",
    "        }\n",
    "    }\n",
    "])\n",
    "\n",
    "context = \"\"\n",
    "for document in results:\n",
    "    print(type(document))\n",
    "    print(f'Text present: {document[\"text\"]}\\n')\n",
    "    context += document[\"text\"]\n",
    "\n",
    "print(\"Query worked\")\n",
    "print(context)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "\n",
    "print(\"Pipeline code worked\")\n",
    "\n",
    "\n",
    "def prompt_tinyllama(prompt, system_prompt=\"\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]\n",
    "    prompt = pipe.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    outputs = pipe(prompt, max_new_tokens=256, do_sample=True, temperature=0.7, top_k=50, top_p=0.95)\n",
    "    return outputs[0][\"generated_text\"].split(\"<|assistant|>\")[1]\n",
    "\n",
    "\n",
    "prompt = f\"With the following context- {context}\\nAnswer the following query {query}\"\n",
    "print(\"Querying: \"+ prompt)\n",
    "system_prompt = \"You are an expert in this field and always provide detailed and accurate answers\"\n",
    "response = prompt_tinyllama(prompt, system_prompt)\n",
    "print(response)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
